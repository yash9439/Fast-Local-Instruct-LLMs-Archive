{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlL4NDLNynOk",
        "outputId": "212cd925-3074-448b-a0bd-47b23984edfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51Pkq_fC0G-W",
        "outputId": "693a9494-9b1f-44a5-f54e-d7afc2dd0d54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement datsets (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for datsets\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U datsets transformers[sentencepiece]\n",
        "!pip install -q -U sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oqDVNSeyyeI8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETqbm-yxzW-F",
        "outputId": "6edf4fab-e61d-439a-b75f-dbabc427a37e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        }
      ],
      "source": [
        "model_path = 'openlm-research/open_llama_3b'\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSOWtR_azYGb",
        "outputId": "098a3a4d-a93c-4e06-e282-c05f6127779f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>Q: What is the largest animal?\n",
            "A: The blue whale.\n",
            "Q: What is the largest animal?\n",
            "A: The blue whale. It is the largest animal on Earth. It is also the largest mammal. It is the largest creature that has ever lived.\n",
            "Q: What is the largest animal?\n",
            "A: The blue whale is the largest animal on Earth. It is also the largest mammal. It is the largest creature that has ever lived.\n",
            "Q: What is the largest animal?\n",
            "A: The blue whale is the largest animal on Earth. It is also the largest mammal. It is the largest creature that has ever lived.\n",
            "Q: What is the largest animal?\n",
            "A: The blue whale is the largest animal on Earth. It is also the largest mammal. It is the largest creature that has ever lived.\n",
            "Q: What is the largest animal?\n",
            "A: The blue whale is the largest animal on Earth. It is also the largest mammal. It is the largest creature that has ever lived.\n",
            "Q: What is the largest animal?\n",
            "A: The blue whale is the largest animal on Earth. It is also the largest mammal. It is the largest creature that has ever lived.\n",
            "Q: What is the largest animal?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = 'Q: What is the largest animal?\\nA:'\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "generation_output = model.generate(\n",
        "    input_ids=input_ids, max_new_tokens=256\n",
        ")\n",
        "print(tokenizer.decode(generation_output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcFyBviUzc6e",
        "outputId": "0d731811-592f-4272-c08c-9ec176ba7095"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Read the JSON data from file\n",
        "with open('questions.json', 'r', encoding='utf-8') as file:\n",
        "    json_data = json.load(file)\n",
        "\n",
        "# Remove data points with the same query_string\n",
        "unique_entries = []\n",
        "query_strings_seen = set()\n",
        "\n",
        "for entry in json_data:\n",
        "    query_string = entry['query_string']\n",
        "\n",
        "    if query_string not in query_strings_seen:\n",
        "        unique_entries.append(entry)\n",
        "        query_strings_seen.add(query_string)\n",
        "\n",
        "# Open the file to append the data\n",
        "with open('open_llama_3b.txt', 'a') as output_file:\n",
        "    ct = 1\n",
        "\n",
        "    # Iterate over the unique data entries and extract relevant information\n",
        "    for entry in unique_entries:\n",
        "        question = entry['query_string']\n",
        "        prompt = f'Q: {question}\\nA:'\n",
        "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids, max_new_tokens=128\n",
        "        )\n",
        "        # print(tokenizer.decode(generation_output[0]))\n",
        "\n",
        "        generated_text = tokenizer.decode(generation_output[0])  # Assuming generate_text() returns the generated text\n",
        "\n",
        "        # Write query_string and generated_text to the file\n",
        "        output_file.write(f'{generated_text}\\n\\n')\n",
        "        print(ct)\n",
        "        ct += 1\n",
        "        print(generated_text)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
